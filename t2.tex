\section{Runge-Kutta and Linear Multistep Methods}

\subsection{General Runge-Kutta methods}

Runge-Kutta methods are a family of iterative methods, which include the previously seen Euler's Method. Let's define an RK method with $s$ stages:\\

Given $x\in \real, \, y\in\real^n,\, y'=f(x,y)$

\begin{definition}
  The \textbf{Butcher Tableau} is 
  $$\begin{array}{c|c c c c}
     c_1    & a_{11} & a_{12} & \ldots  & a_{1s} \\
     c_2    & a_{21} & a_{22} &         & \vdots \\
     \vdots & \vdots &        & \ddots  & \vdots \\
     c_s    & a_{s1} & \hdotsfor{2}     & a_{ss} \\
     \hline
            & b_1    & \hdotsfor{2}     & b_s
  \end{array}$$
\end{definition}

With these coefficients given by the table, we can now define our Runge-Kutta method:

$$
\begin{rcases}
  k_1 = f\Big(x_n + c_1h, y_n + h(a_{11}k_1 + a_{12}k_2 + \ldots + a_{1s}k_s)\Big)\\
  k_2 = f\Big(x_n + c_2h, y_n + h(a_{21}k_1 + a_{22}k2 + \ldots + a_{2s}k_s)\Big) \\
  \hspace{0.15cm}\vdots \\
  k_s = f\Big(x_n + c_sh, y_n + h(a_{s1}k_1 + a_{s2}k2 + \ldots + a_{ss}k_s)\Big)
\end{rcases} \substack{\text{System of equations} \\ \text{with unknowns } k_1, k_2, \ldots, k_s}
$$\-\\

$$\boxed{y_{n+1} = y_n + h(b_1k_1 + b_2k_2 + \ldots + b_sk_s)}$$

\-\\

\underline{Cases}:
\begin{enumerate}[label = (\arabic*)]
    \item Explicit ($a_{ij} = 0$ for $j\geq i$ and $c_1 = 0$)
    \item Semi-implicit ($a_{ij} = 0$ for $j>i$)
    \item Implicit\-\\
\end{enumerate}
\begin{remark}
  (2) and (3) are used for stiff problems.
\end{remark}

\newpage

\begin{theorem}
  An explicit $s$-stage Runge-Kutta method can't have order $> 5$.
\end{theorem}

\begin{theorem}
  There is no explicit $5$-stage RK of order $5$.
\end{theorem}

\begin{theorem}\-\\

  Let \\
  
  \-\hspace{0.5cm}$A = \text{order } p \text{ for } y'=f(y), \, f:\real^m \to \real^m, \, m>1$\\
  \-\hspace{0.5cm}$B = \text{order } p \text{ for } y'=f(x,y), \, f:\real\times\real \to \real$\\
  \-\hspace{0.5cm}$C = \text{order } p \text{ for } y'=f(y), \, f:\real \to \real$\\
  
  Then\\
  
  \-\hspace{0.5cm}$\text{For }1\leq p\leq3, \, A\iff B \iff C$ \\
  \-\hspace{0.5cm}$\text{For }p = 4,        \, A\iff B \implies C \, \text{ but } C \centernot\implies B$\\
  \-\hspace{0.5cm}$\text{For }p \geq 5,     \, A\hspace{0.1cm}\implies B \implies C \, \text{ but } C \centernot\implies B, \, B \centernot\implies A$\\
\end{theorem}

\subsubsection{Embedded Runge-Kutta methods}

Embedded methods combine two methods of order $p$ and $p+1$ to obtain an estimate of the LTE. This error estimate is useful for adaptive stepsize methods. \\

In this case, the \textit{Butcher tableau} is

$$\begin{array}{c|c c c c c}
     0                                                                   \\
     c_2    & a_{21}                                                     \\
     c_3    & a_{31}            & a_{32}                                 \\
     \vdots & \vdots            &                  & \ddots              \\
     c_s    & a_{s1}            & \hdotsfor{2}               & a_{s,s-1} \\
     \hline
            & b_1               & \hdotsfor{2}               & b_{s-1}   & b_s  \\
            & \overline{b}_1    & \hdotsfor{2}               & \overline{b}_{s-1}  &\overline{b}_s
\end{array}$$\-\\
This kind of method has the advantage that it usually requires less steps even though there are more evaluations for each step.\\

Matlab's current \texttt{ode45} solver uses an embedded RK method with orders $4$ and $5$.

\vspace{-3.5cm}\hspace{11.2cm}order $p$

\vspace{+0.05cm}\hspace{11.2cm}order $p+1$

\newpage
\subsection{Linear multistep methods}

\begin{definition}
  A \textbf{linear k-step method} is a method of the form
  
  \[
    \sum_{j=0^k}\alpha_jy_{n+j} = h\sum_{j=0}^k\beta_jf_{n+j}
  \]
  
  with $\alpha_j,\,\beta_j\in\real, \quad \alpha_k = 1$ and $\alpha_0 \not=0 \, or \, \beta_0\not= 0$. \\
\end{definition}

\underline{Notation}: $f_m = f(x_m, y_m), \, x_m = x_0 + mh$

\begin{remark}
  If $\beta_k = 0$, then the method is explicit.
\end{remark}

\begin{remark}
  With these methods, adapting the step size can be complicated. It may even change the order.
\end{remark}

\begin{example}
    Our initial value problem is
    \[
      \begin{cases}
        y' = f(x,y) \\
        y_0 = y(x_0)
      \end{cases}
    \]
    And we want to solve it using the following multistep method:
    \[
      y_{n+2} - y_n = \frac{h}{2}(f_{n+2} + 4f_{n+1} + f_n)
    \]
    
    How do we compute $y_1$? \\
    
    Usually we first use a one-step method like Euler's to compute the initial terms needed, and then we apply the multistep method.
\end{example}

\begin{example}

Let's see how we compute the order of $y_{n+2}-y_n = h(\beta_1f_{n+1} + \beta_0f_n)$:

\[
  \text{LTE} = \norm{y(x_{n+2}) - y_{n+2}}
\]

\begin{align*}
    y(x_n + 2h) - y_{n+2} &= \cancel{y(x_n)} + 2hy'(x_n) + \frac{1}{2}y''(x_n)4h^2 + \frac{1}{3!}y'''(x_n)8h^3 + \mathcal{O}(h^4) - \\
    &\hspace{0.5cm}- \Big(\cancel{y(x_n)} + h\beta_1f(y_{n+1}) + h\beta_0f(y_n) \Big) \\
    &= 2hy'(x_n) + \frac{1}{2}y''(x_n)4h^2 + \frac{1}{3!}y'''(x_n)8h^3 + \mathcal{O}(h^4) - \\
    &\hspace{0.5cm}- \Big(h\beta_1\hspace{-0.5cm}\underbrace{y'(x_n + h)}_{y'(x_n) + hy''(x_n) + \ldots}\hspace{-0.4cm} + h\beta_0y'(x_n) \Big) \\
\end{align*}

\newpage

Grouping by powers of $h$ we get:\\

$h \hspace{0.2cm}\longrightarrow 2y'(x_n) - \beta_1y'(x_n) - \beta_0y'(x_n) = 0 \implies \beta_0 + \beta_1 = 2$\\

$h^2 \longrightarrow 2y''(x_n) - \beta_1y''(x_n) = 0 \implies b_1 = 2 \implies \beta_0 = 0$\\

So LTE $= \mathcal{O}(h^3)$\\
\end{example}

\subsubsection{Generalities}

\begin{itemize}
    \item \textbf{Adams–Bashforth methods}: These are explicit methods with coefficients $\alpha_{k-1} = -1$ and $\alpha_{k-2} = \alpha_{k-3} = \ldots = \alpha_0 = 0$. The $\beta_j$'s are chosen such that the method has order $k$ ($j = 0, \ldots, k$).\\
    
    For example, the Adams–Bashforth method with $k=2$ is $$y_{n+2} = y_{n+1} + h\left(\frac{3}{2}f_{n+1} - \frac{1}{2}f_n\right)$$
    
    \item \textbf{Adams–Moulton methods}: These have the same coefficients as the Adams–Bashforth methods, but they are implicit methods. Also, for a $k$-step Adams–Moulton method, if $\beta_k \not= 0$, it can reach order $k+1$, while an A-B method has only order $k$.\\
    
    The Adams–Moulton method with $k = 2$ is $$y_{n+2} = y_{n+1} + h\left(\frac{5}{12}f_{n+2} + \frac{2}{3}f_{n+1} - \frac{1}{12}f_n \right)$$
    
    \item \textbf{Backward differentiation formulas (BDF)}: These are implicit methods with $b_{k-1} = \ldots = b_{0} = 0$ and the rest of the coefficients are chosen such that the method has order $k$ (only exist for $k<7$). BDF methods are useful for stiff problems.
    
    \item \textbf{Predictor-Corrector method}: A Predictor-Corrector method uses an explicit method for the predictor step (P) and an implicit method for the corrector step (C). They usually have the same order. 
    $$y_n \underset{P}{\longrightarrow}y_{n+1}^* \underset{C}{\longrightarrow}y_{n+1}$$
    
    They have the advantage that the LTE is easy to estimate ($\abs{predictor\,-\,corrector}$)\\
    The improved Euler's method (\ref{imprEu}) is an example of a P-C method.
\end{itemize}

\newpage
\subsubsection{Richardson's extrapolation}

Richardson's extrapolation is used to improve the rate of convergence of a method.\\

Suppose we have a one-step explicit method. Then
\begin{itemize}
    \item Compute one step with stepsize $h$: $y_n \to y_{n+1}$
    \item Compute two steps with stepsize $\frac{h}{2}$: $y_n \to y_{n+\frac{1}{2}} \to \hat{y}_{n+1}$
\end{itemize}

Then  $$\text{LTE} \simeq \norm{y_{n+1} - \hat{y}_{n+1}}$$

If the method is of order $p$, we have
\begin{align}
    y(x_{n+1}) - y_{n+1} &= Ch^{p+1} + \mathcal{O}(h^{p+2})\\
    y(x_{n+1}) - \hat{y}_{n+1} &= 2C\left(\frac{h}{2}\right)^{p+1} + \mathcal{O}(h^{p+2})
\end{align}
      
And subtracting (2.1) - (2.2), we end up with

$$\hat{y}_{n+1} - y_{n+1} = C\left(h^{p+1}-2\frac{h^{p+1}}{2^{p+1}}\right) = Ch^{p+1}\underbrace{\left(1-\frac{1}{2^p}\right)}_{\dfrac{2^p-1}{2^p}} \simeq \text{LTE}$$

and $$C = \frac{\norm{\hat{y}_{n+1}-y_{n+1}}}{h^{p+1}}\cdot\frac{2^p}{2^p-1}$$

Now, while applying our method, we want to keep the LTE below a tolerance $TOL$. If the LTE is larger than $TOL$, we want to reduce the stepsize, and if it's significantly smaller, we don't want to waste computational time, so we should increase the stepsize. So an automatic adjustment of $h$ can be computed as follows: \\

Let $h^*$ be a new stepsize such that the LTE is equal to the tolerance ($C\cdot(h^*)^{p+1} = TOL$).\\

Then $$(h^*)^{p+1} = \frac{TOL}{C} = \frac{TOL}{\norm{\hat{y}_{n+1} - y_{n+1}}}\cdot\frac{2^p-1}{2^p}h^{p+1}$$

And the new stepsize is given by $$h^* = 0.9h\left(\frac{TOL}{\norm{\hat{y}_{n+1} - y_{n+1}}}\cdot\frac{2^p-1}{2^p}\right)^{\frac{1}{p+1}}$$

\newpage
\subsubsection{Convergence of a linear multistep method}

\begin{theorem}
  Given a general linear multistep method $$y_{n+k} + \alpha_{k-1}y_{n+k-1} + \ldots + \alpha_0y_n = h\big(\beta_kf_{n+k}+\ldots+\beta_0f_n \big)$$ 
  
  and the polynomials
  \begin{align*}
      \rho(z)   &= z^k + a_{k-1}z^{k-1} + \ldots + a_0\\
      \sigma(z) &= \beta_kz^k + \beta_{k-1}z^{k-1} + \ldots + \beta_0
  \end{align*}
  
  Then, a necessary and sufficient condition for this method to be convergent is:
  \begin{enumerate}
      \item It is of order 1 at least.
      \item The roots of the first stability polynomial ($\rho(z)$) are on the unit ball, and if they are on the boundary, they are simple (as roots).
  \end{enumerate}
\end{theorem}

\begin{remark}
  $1$ is always a root of $\rho(z)$.
\end{remark}
\-\\
Let's see an example of divergence using a linear multistep method:

\begin{example}
    \-\\
    \textit{\textbf{Given the method}}

    \[
      \boldsymbol{y_{n+2} + a_1y_{n+1} + a_0y_n = h(b_1f_{n+1} + b_0f_n)}
    \]
    
    \textit{\textbf{1) Find $\boldsymbol{a_0,a_1,b_0,b_1}$ so that the method above has the highest possible order}}.\\
    
   \textit{\textbf{2) Try it on $$\boldsymbol{\begin{cases}y' = -y \\ y(0) = 1\end{cases}}\quad (\boldsymbol{y_0=1, y_1=e^{-h}})$$
    and prove the method diverges.}}\\
    
    1) We want $y(x_n + 2h) - y_{n+2}$ \\
    
    We assume $y_{n+1} = y(x_n + h), y_n = y(x_n)$ (localizing assumption).
    
    
    \begin{align*}
        y(x_n + 2h) - y_{n+2} &= y(x_n + 2h) - \Big[-a_1y_{n+1} - a_0y_n + h\big(b_1f(y_{n+1}) + b_0f(y_n)\big) \Big]\underset{\text{loc.as.}}{=}\\
        &= y(x_n + 2h) - \Big[-a_1y(x_n+h) - a_0y(x_n) + hb_1\underbrace{f\left(y(x_n+h)\right)}_{y'(x_n + h)} + hb_0\underbrace{f\left(y(x_n)\right)}_{y'(x_n)} \Big]
    \end{align*}
    
    \newpage
    As usual, we expand in powers of $h$. We'll expand to order 3
    
    \begin{align*}
        y(x_n) &+ 2hy'(x_n) + \frac{4h^2}{2}y''(x_n) + \frac{8h^3}{6}y'''(x_n) + o(h^4) - \\
        -\bigg[ &-a_1\left(y(x_n) + hy'(x_n) + \frac{h^2}{2}y''(x_n) + \frac{h^3}{6}y'''(x_n) + o(h^4)\right) - \\
        &-a_0y(x_n) \\
        &+hb_1\left(y'(x_n) + hy''(x_n) + \frac{h^2}{2}y'''(x_n) + o(h^3) \right) + \\
        &+hb_0y'(x_n)\bigg]
    \end{align*}
    
    Let's group by powers of $h$ and assume the right conditions to obtain the highest possible order:\\
    
    $h^0 \longrightarrow y(x_n) + a_1y(x_n) + a_0y(x_n) = 0$ \\
    
    $h^1 \longrightarrow 2hy'(x_n) + a_1hy'(x_n) - hb_1y'(x_n) - hb_0y'(x_n) = 0$ \\
    
    $h^2 \longrightarrow 2h^2y''(x_n) + a_1\dfrac{1}{2}h^2y''(x_n) - b_1h^2y''(x_n) = 0$\\
    
    $h^3 \longrightarrow \dfrac{8h^3}{6}y'''(x_n) + a_1\dfrac{h^3}{6}y'''(x_n) - b_1h\left(\dfrac{h^2}{2}y'''(x_n)\right) = 0$ \\
    
    With that, we get the system of equations
    
    $$
      \begin{cases}
        1 + a_1 + a_0 = 0\\
        2 + a_1 - b_1 - b_0 = 0 \vspace{0.1cm}\\
        2 + \dfrac{a_1}{2} - b_1 = 0 \vspace{0.1cm}\\
        \dfrac{8}{6} + \dfrac{a_1}{6} - \dfrac{b_1}{2} = 0
      \end{cases}
    $$
    
    And we end up with
    
    \[
      a_0 = -5, \quad a_1 = 4, \quad b_0 = 2, \quad b_1 = 4
    \]
    
    %\vspace{1cm}
    \newpage
    
    2)  Our method is 
    $$y_{n+2} + 4y_{n+1} - 5y_n = h(4f_{n+1} + 2f_n)$$
    
    and with $$\begin{cases}y' = -y \\y(0) = 1, \, y(h) = e^{-h} \end{cases} \qquad (y(x) = e^{-x})$$ we have
    
    \[
      y_{n+2} + 4y_{n+1} - 5y_n = h(-4y_{n+1} - 2y_n)
    \]
    
    We'll find a solution of the form $$y_n = c_1(\quad)^n + c_2(\quad)^n$$ and we'll see that it diverges.
    
    \begin{align*}
      &\lambda^2 + 4\lambda - 5 + 4h\lambda + 2h = 0\\
      &\lambda^2 + (4(1+h))\lambda + (2h-5) = 0\\
    \end{align*}
    \[
      \lambda = \frac{-4(1+h) \pm \sqrt{4^2(1+h)^2 - 4(2h-5)}}{2}
    \]
    
    Let's expand the discriminant
    
    \begin{align*}
        \sqrt{4^2(1 + 2h + h^2) - 8h + 20} &= \sqrt{36 + 24h +16h^2} = 6\sqrt{1+\frac{4}{6}h + \frac{4^2}{6^2}h^2} \underset{\text{Taylor}}{=}\\
        &=6\left(1 + \frac{1}{2}\left(\frac{4}{6}h + \frac{4^2}{6^2}h^2\right) + o(h^2)\right) =\\
        &=6\left(1 + \frac{1}{3}h + o(h^2)\right)
    \end{align*}
      
    So
    
    $$
        \lambda = \frac{-4 - 4h \pm (6 + 2h + o(h^2))}{2}
          \begin{tikzcd}[cramped, sep=tiny]
                                    & \hspace{0.6cm}1-h+o(h^2) \\
            = \arrow[ur] \arrow[dr] &            \\
                                    & \hspace{0.8cm}-5-3h + o(h^2)
          \end{tikzcd}
    $$
    
    $\implies y_n = c_1\big(1-h+o(h^2)\big)^n + c_2\big(-5-3h+o(h^2)\big)^n$\\
    
    \newpage
    Let's find $c_1$ and $c_2$ imposing the initial conditions
    
    \[
    \begin{cases}
        1=c_1+c_2 \implies c_1 = 1-c_2 \\
        e^{-h} = c_1\big(1-h+o(h^2)\big) + c_2\big(-5-3h+o(h^2)\big)
    \end{cases}
    \]
    
    \[
       \implies e^{-h} = (1-h+o(h^2)) + c_2\big(\underbrace{-5-3h+o(h^2) - 1 + h + o(h^2)}_{-6-2h+o(h^2)}\big)
    \]
    \huge{\textbf{Ignore the following part. \\Needs review}}
    \normalsize
    \[
      \implies c_2 = \frac{e^{-h} - 1 + h +o(h^2)}{-6-2h+o(h^2)} \underset{\text{Taylor }e^{-h}}{=} \frac{1-h+o(h^2) - 1 + h +o(h^2)}{-6-2h+o(h^2)} \simeq \frac{-1}{6+2h}
    \]
    
    \[
      \implies c_1 \simeq 1 + \frac{1}{6+2h} = \frac{7+2h}{6+2h}
    \]
    
    \-\\So $$y_n = \frac{7+2h}{6+2h}\big(1-h+o(h^2)\big)^n + \frac{-1}{6+2h}\big(-5-3h+o(h^2)\big)^n$$
    
    \-\\and the term $(-5)^n$ will cause the solution to diverge.
\end{example}